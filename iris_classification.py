# -*- coding: utf-8 -*-
"""iris_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1opJXNx3Cl0zGeC5cuVuzh7B6o9j-1ZL2

# 手把手機器學習 (下)

## Iris Dataset
"""

from sklearn.datasets import load_iris
iris = load_iris()

print(iris.keys())

print(iris.feature_names)
print(iris.target_names)

print(iris.DESCR)

print(iris.data.shape)
print(iris.data[0:5])

print(iris.target.shape)
print(iris.target)

"""##EDA"""

import pandas as pd

X = iris.data
y = iris.target
df = pd.DataFrame(X, columns = iris.feature_names)
df['Species'] = y
df.head()

"""## 訓練集 / 測試集"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,test_size=0.3, random_state= 12)

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train,y_train)
y_test_pred = model.predict(X_test)

"""# Confusion Matrix"""

from sklearn import metrics

print('Accuracy:%.2f'%metrics.accuracy_score(y_test, y_test_pred))
print(metrics.confusion_matrix(y_test, y_test_pred))

y_test_pred = model.predict(X_test)
y_test_pred_proba = model.predict_proba(X_test)
print('Classification Output:\n', y_test_pred)
print('Probability Output:\n', y_test_pred_proba)

"""# Tuning Logistic Regression - C Parameter"""

model = LogisticRegression(C=0.2)
model.fit(X_train,y_train)
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)
print('train evaluation---------------------------------')
print('accuracy:%.2f'%metrics.accuracy_score(y_train, y_train_pred))
print('confusion matirx:\n', metrics.confusion_matrix(y_train, y_train_pred))
print('test evaluation---------------------------------')
print('accuracy:%.2f'%metrics.accuracy_score(y_test, y_test_pred))
print('confusion matirx:\n', metrics.confusion_matrix(y_test, y_test_pred))

"""# Tuning Logistic Regression - Multi_Class"""

model = LogisticRegression(solver='sag', multi_class='multinomial',max_iter=10)
model.fit(X_train,y_train)
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)
print('train evaluation---------------------------------')
print('accuracy:%.2f'%metrics.accuracy_score(y_train, y_train_pred))
print('confusion matirx:\n', metrics.confusion_matrix(y_train, y_train_pred))
print('test evaluation---------------------------------')
print('accuracy:%.2f'%metrics.accuracy_score(y_test, y_test_pred))
print('confusion matirx:\n', metrics.confusion_matrix(y_test, y_test_pred))

"""# SVM"""

import numpy as np
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
# plot 2D decision regions
def plot_decision_regions(X, y, classifier, test_idx = None, resolution=0.02):
    # setup marker generator and color map
    markers = ('s','x','o','^','v')
    colors = ('red','blue','lightgreen','gray','cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])

    # plot the decision surface
    x1_min, x1_max = X[:,0].min() - 1, X[:,0].max() + 1
    x2_min, x2_max = X[:,1].min() - 1, X[:,1].max() + 1

    xx1, xx2 = np.meshgrid(np.arange(x1_min,x1_max,resolution),
                            np.arange(x2_min,x2_max,resolution))

    Z = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)

    Z = Z.reshape(xx1.shape)

    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
    plt.xlim(xx1.min(),xx1.max())
    plt.ylim(xx2.min(),xx2.max())

    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y==cl,0], y=X[y==cl,1],
            alpha=0.8, c=cmap(idx), marker=markers[idx],label=cl)
    if test_idx:
        X_test, y_test = X[test_idx,:], y[test_idx]
        plt.scatter(X_test[:, 0], X_test[:,1], c='',
            alpha=1.0, linewidth=1, marker='o',
            s=55, label='test set')

"""## SVM - Kernel"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X = iris.data[:, 0:2] # select sepal length, sepal width only
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.svm import SVC
# kernel: linear, poly, rbf
model = SVC(kernel='poly')
model.fit(X_train, y_train)
y_predict = model.predict(X_test)
accuracy = model.score(X_test,y_test)
print('accuracy: ', accuracy)
# 另一種寫法
# from sklearn.metrics import accuracy_score
# accuracy = accuracy_score(y_predict, y_test)
plot_decision_regions(X_test, y_test, model)

data_test = [[5,3.5], [5,2], [7,3.5]]
print(model.predict(data_test))
print(model.predict_proba(data_test))

"""## SVM - Input feature dimension"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X = iris.data[:, [0,1,2]]
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
from sklearn.svm import SVC
# kernel: linear, poly, rbf
model = SVC(kernel='rbf')
model.fit(X_train, y_train)
accuracy = model.score(X_test,y_test)
print('accuracy: ', accuracy)
# plot_decision_regions(X_test, y_test, model) # 2 features only

"""## SVM - C: penalty"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X = iris.data[:, [0,1]]
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
from sklearn.svm import SVC
# kernel: linear, poly, rbf
model = SVC(kernel='rbf', C=10)
model.fit(X_train, y_train)
accuracy = model.score(X_test,y_test)
print('accuracy: ', accuracy)
plot_decision_regions(X_test, y_test, model) # 2 features only

"""## SVM - Regression

### Generate Data: sin(x)
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# define input array with angles from 60deg to 300deg converted to radians
def generate_sin():
    X = np.array([i*np.pi/180 for i in range(60,300,4)])
    np.random.seed(100)  #Setting seed for reproducability
    y = np.sin(X) + np.random.normal(0,0.15,len(X))
    X = X.reshape(60,-1)
    y = y.reshape(60,-1)
    data = pd.DataFrame(np.column_stack([X,y]),columns=['X','y'])
    plt.plot(data['X'],data['y'],'.')
    return X, y

X, y = generate_sin()

"""### Tunning kernel, C"""

from sklearn.metrics import mean_absolute_error
from sklearn.svm import SVR
def svm_regression(X, y, kernel='rbf', C=1):
    model = SVR(kernel=kernel, C=C)
    model.fit(X, y)
    y_pred = model.predict(X)
    mae = mean_absolute_error(y_pred, y)
    plt.plot(X, y,'.')
    plt.plot(X ,y_pred)
    print('mae error: ', mae)

# kernel: rbf, poly, linear
svm_regression(X, y, kernel='rbf', C=1)

"""# Decision Tree"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
iris = load_iris()
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target)
tree = DecisionTreeClassifier()
tree.fit(x_train, y_train)
y_pred = tree.predict(x_test)
accuracy_score(y_test, y_pred)

# feature importance
tree.feature_importances_

"""## 看決策樹怎麼分類"""

# install packages
!apt-get install python-pydot
!pip install pydotplus

import pydotplus
from IPython.display import Image
from sklearn.tree import export_graphviz
dot_data = export_graphviz(tree, 
                           feature_names=iris.feature_names, 
                           class_names=iris.target_names, 
                           out_file=None) 
# Draw graph
graph = pydotplus.graph_from_dot_data(dot_data)  
# Show graph
Image(graph.create_png())

"""### 練習 決策樹參數"""

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
iris = load_iris()
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target)

tree = DecisionTreeClassifier(max_depth=100, 
                              min_samples_split=2, 
                              min_samples_leaf=1)
tree.fit(x_train, y_train)
y_pred = tree.predict(x_test)
accuracy_score(y_test, y_pred)

"""# Random Forest 隨機森林"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
iris = load_iris()
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random)

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, 
                              max_depth=None,
                              min_samples_split=2, 
                              min_samples_leaf=1)
rf.fit(x_train, y_train)
y_pred = tree.predict(x_test)
accuracy_score(y_test, y_pred)

"""## Gradient Boosting Machine"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
iris = load_iris()
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target)

from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(n_estimators=100, 
                                learning_rate=0.1,
                                max_features=None,
                                max_depth=3)
gb.fit(x_train, y_train)
y_pred = gb.predict(x_test)
accuracy_score(y_test, y_pred)

"""## XGBoost"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
iris = load_iris()
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target)

from xgboost import XGBClassifier 
xgb = XGBClassifier(booster='gbtree', max_depth=3, learning_rate=0.1, n_estimators=100)
xgb.fit(x_train, y_train, eval_set=[(x_test, y_test)])
y_pred = gb.predict(x_test)
accuracy_score(y_test, y_pred)

from xgboost import plot_importance
plot_importance(xgb)

!pip install graphviz
from xgboost import plot_tree
import matplotlib
plot_tree(xgb, num_trees=5)
matplotlib.pyplot.gcf().set_size_inches(10, 10)